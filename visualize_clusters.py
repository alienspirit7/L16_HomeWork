#!/usr/bin/env python3
"""
Create 2D visualizations of title embeddings using PCA.
Point colors represent K-Means clusters, marker shapes denote original groups,
and cluster centroids are annotated for context.
"""
from __future__ import annotations

import argparse
import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA


SUPPORTED_DATA_FORMATS = {".parquet", ".csv", ".jsonl"}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Visualize embeddings with colors mapped to K-Means clusters and shapes to original groups.")
    parser.add_argument("--data", required=True, help="Dataset generated by prepare_embeddings.py.")
    parser.add_argument("--assignments", required=True, help="Clustering results (from run_kmeans.py) to determine cluster colors.")
    parser.add_argument(
        "--output",
        required=True,
        help="Path for the generated visualization (PNG).",
    )
    parser.add_argument(
        "--title",
        default="Title Embeddings (PCA Projection)",
        help="Plot title.",
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=0.8,
        help="Point transparency (0-1).",
    )
    parser.add_argument(
        "--log-level",
        default="info",
        choices=["debug", "info", "warning", "error", "critical"],
        help="Logging verbosity.",
    )
    return parser.parse_args()


def configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(asctime)s %(levelname)s %(message)s",
    )


def load_dataset(path: Path) -> pd.DataFrame:
    ext = path.suffix.lower()
    if ext not in SUPPORTED_DATA_FORMATS:
        raise ValueError(f"Unsupported dataset format '{ext}'. Expected one of {sorted(SUPPORTED_DATA_FORMATS)}")
    if ext == ".parquet":
        df = pd.read_parquet(path)
    elif ext == ".csv":
        df = pd.read_csv(path)
    else:
        records: List[Dict[str, Any]] = []
        with path.open("r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if line:
                    records.append(json.loads(line))
        df = pd.DataFrame(records)
    required_cols = {"title", "original_group", "normalized_embedding"}
    missing = required_cols - set(df.columns)
    if missing:
        raise ValueError(f"Dataset missing required columns: {sorted(missing)}")
    return df.reset_index(drop=True)


def ensure_vector(value: Any) -> List[float]:
    if isinstance(value, list):
        return [float(v) for v in value]
    if isinstance(value, np.ndarray):
        return value.astype(float).tolist()
    if isinstance(value, str):
        try:
            parsed = json.loads(value)
        except json.JSONDecodeError:
            from ast import literal_eval

            parsed = literal_eval(value)
        if not isinstance(parsed, (list, tuple)):
            raise ValueError(f"Expected list-like string, got {type(parsed)}")
        return [float(v) for v in parsed]
    if isinstance(value, tuple):
        return [float(v) for v in value]
    raise TypeError(f"Unsupported vector type: {type(value)}")


def load_assignments(path: Path) -> pd.DataFrame:
    supported = {".csv", ".parquet", ".jsonl"}
    ext = path.suffix.lower()
    if ext not in supported:
        raise ValueError(f"Unsupported assignments format '{ext}'. Expected CSV, Parquet, or JSONL.")
    if ext == ".csv":
        df = pd.read_csv(path)
    elif ext == ".parquet":
        df = pd.read_parquet(path)
    else:
        records: List[Dict[str, Any]] = []
        with path.open("r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if line:
                    records.append(json.loads(line))
        df = pd.DataFrame(records)
    if "cluster_id" not in df.columns:
        raise ValueError("Assignments file must contain 'cluster_id' column.")
    return df[["title", "cluster_id"]]


def reduce_dimensions(vectors: np.ndarray) -> np.ndarray:
    if vectors.shape[1] <= 2:
        logging.info("Vectors already 2D; skipping PCA.")
        return vectors
    pca = PCA(n_components=2, random_state=42)
    reduced = pca.fit_transform(vectors)
    explained = pca.explained_variance_ratio_.sum()
    logging.info("PCA reduced to 2D (explained variance: %.2f%%)", explained * 100)
    return reduced


def main() -> None:
    args = parse_args()
    configure_logging(args.log_level)

    data_path = Path(args.data)
    assign_path = Path(args.assignments)
    output_path = Path(args.output).resolve()
    output_path.parent.mkdir(parents=True, exist_ok=True)

    df = load_dataset(data_path)
    vectors = np.asarray([ensure_vector(v) for v in df["normalized_embedding"]], dtype=float)
    reduced = reduce_dimensions(vectors)

    df["x"] = reduced[:, 0]
    df["y"] = reduced[:, 1]

    assignments = load_assignments(assign_path)
    cluster_map = dict(zip(assignments["title"], assignments["cluster_id"]))
    df["cluster_id"] = df["title"].map(cluster_map)
    if df["cluster_id"].isna().any():
        missing = df[df["cluster_id"].isna()]["title"].tolist()
        raise ValueError(f"Missing cluster assignments for titles: {missing[:5]}")

    color_palette = plt.get_cmap("Set1")
    unique_clusters = sorted(df["cluster_id"].dropna().unique())
    cluster_colors = {cluster_id: color_palette(idx % color_palette.N) for idx, cluster_id in enumerate(unique_clusters)}

    original_groups = sorted(df["original_group"].unique())
    markers = ["o", "^", "s", "D", "P", "X", "v", "<", ">", "h", "*"]
    group_markers = {group: markers[idx % len(markers)] for idx, group in enumerate(original_groups)}

    fig, ax = plt.subplots(figsize=(10, 8))

    for group, group_df in df.groupby("original_group"):
        marker = group_markers[group]
        colors = [cluster_colors.get(cluster_id, "#333333") for cluster_id in group_df["cluster_id"]]
        ax.scatter(
            group_df["x"],
            group_df["y"],
            c=colors,
            marker=marker,
            label=group,
            alpha=args.alpha,
            edgecolor="black",
            linewidths=0.4,
        )

    for cluster_id, cluster_df in df.groupby("cluster_id"):
        centroid_x = cluster_df["x"].mean()
        centroid_y = cluster_df["y"].mean()
        ax.text(
            centroid_x,
            centroid_y,
            f"C{cluster_id}",
            fontsize=10,
            fontweight="bold",
            ha="center",
            va="center",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.6),
        )

    ax.set_title(args.title)
    ax.set_xlabel("Component 1")
    ax.set_ylabel("Component 2")
    group_handles = [
        Line2D(
            [0],
            [0],
            marker=group_markers[group],
            color="black",
            markerfacecolor="white",
            markersize=8,
            linestyle="",
            label=group,
        )
        for group in original_groups
    ]

    cluster_handles = [
        Line2D(
            [0],
            [0],
            marker="o",
            color="white",
            markerfacecolor=cluster_colors[cluster_id],
            markeredgecolor="black",
            markersize=8,
            linestyle="",
            label=f"Cluster {cluster_id}",
        )
        for cluster_id in unique_clusters
    ]

    group_legend = ax.legend(handles=group_handles, title="Original Groups", bbox_to_anchor=(1.02, 1), loc="upper left")
    ax.add_artist(group_legend)
    ax.legend(handles=cluster_handles, title="K-Means Clusters", bbox_to_anchor=(1.02, 0.6), loc="upper left")
    ax.grid(True, linestyle="--", linewidth=0.5, alpha=0.4)
    fig.tight_layout()
    fig.savefig(output_path, dpi=300)
    logging.info("Visualization saved to %s", output_path)


if __name__ == "__main__":
    main()
