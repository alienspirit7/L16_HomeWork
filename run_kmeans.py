#!/usr/bin/env python3
"""
Run K-Means clustering over normalized title embeddings and generate reports.
"""
from __future__ import annotations

import argparse
import json
import logging
import math
import sys
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


supported_extensions = {".parquet", ".csv", ".jsonl"}


@dataclass
class ClusterMetrics:
    k: int
    inertia: float
    silhouette: Optional[float]
    cluster_sizes: Dict[int, int]
    mismatch_rate: float
    majority_mapping: Dict[int, str]


def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run K-Means clustering on normalized embeddings.")
    parser.add_argument("--data", required=True, help="Path to dataset produced by prepare_embeddings.py.")
    parser.add_argument("--k", type=int, default=3, help="Number of clusters (default 3).")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility.")
    parser.add_argument("--output", help="Optional path to write detailed record-level clustering results.")
    parser.add_argument("--report", help="Optional path to write aggregate clustering report (CSV or Markdown).")
    parser.add_argument("--manifest", help="Optional path to manifest JSON generated by prepare_embeddings.py.")
    parser.add_argument("--centroids", help="Optional path to persist cluster centroids as JSON.")
    parser.add_argument(
        "--log-level",
        default="info",
        choices=["debug", "info", "warning", "error", "critical"],
        help="Logging verbosity.",
    )
    return parser.parse_args(argv)


def configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(asctime)s %(levelname)s %(message)s",
    )


def load_dataset(path: Path) -> pd.DataFrame:
    ext = path.suffix.lower()
    if ext not in supported_extensions:
        raise ValueError(f"Unsupported data format '{ext}'. Expected one of {sorted(supported_extensions)}")
    if ext == ".parquet":
        df = pd.read_parquet(path)
    elif ext == ".csv":
        df = pd.read_csv(path)
    else:
        records: List[Dict[str, Any]] = []
        with path.open("r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if not line:
                    continue
                records.append(json.loads(line))
        df = pd.DataFrame(records)
    expected_cols = {"title", "original_group", "normalized_embedding"}
    missing = expected_cols - set(df.columns)
    if missing:
        raise ValueError(f"Dataset missing required columns: {sorted(missing)}")
    return df.reset_index(drop=True)


def ensure_vector_list(value: Any) -> List[float]:
    if isinstance(value, list):
        return [float(v) for v in value]
    if isinstance(value, np.ndarray):
        return value.astype(float).tolist()
    if isinstance(value, str):
        try:
            parsed = json.loads(value)
        except json.JSONDecodeError:
            from ast import literal_eval

            parsed = literal_eval(value)
        if not isinstance(parsed, (list, tuple)):
            raise ValueError(f"Expected list-like string, got {type(parsed)}")
        return [float(v) for v in parsed]
    if isinstance(value, tuple):
        return [float(v) for v in value]
    raise TypeError(f"Unsupported vector type: {type(value)}")


def dataframe_to_matrix(df: pd.DataFrame, column: str) -> np.ndarray:
    vectors = [ensure_vector_list(v) for v in df[column]]
    return np.asarray(vectors, dtype=float)


def compute_majority_mapping(groups: Iterable[str]) -> Tuple[str, float]:
    counter = Counter(groups)
    majority_group, count = counter.most_common(1)[0]
    total = sum(counter.values())
    mismatch_rate = 1.0 - (count / total)
    return majority_group, mismatch_rate


def build_metrics(df: pd.DataFrame, assignments: np.ndarray, kmeans: KMeans) -> ClusterMetrics:
    df = df.copy()
    df["cluster"] = assignments
    cluster_sizes = df["cluster"].value_counts().sort_index().to_dict()

    mismatches = []
    mapping: Dict[int, str] = {}
    for cluster_id, group_df in df.groupby("cluster"):
        majority_group, mismatch_rate = compute_majority_mapping(group_df["original_group"])
        mapping[cluster_id] = majority_group
        mismatches.append(mismatch_rate * len(group_df))
    total_points = len(df)
    mismatch_rate = sum(mismatches) / total_points if total_points else math.nan

    silhouette: Optional[float] = None
    if total_points >= 2 * kmeans.n_clusters:
        try:
            silhouette = float(silhouette_score(
                dataframe_to_matrix(df, "normalized_embedding"),
                assignments,
                metric="cosine",
            ))
        except Exception as exc:  # noqa: BLE001
            logging.warning("Unable to compute silhouette score: %s", exc)

    return ClusterMetrics(
        k=kmeans.n_clusters,
        inertia=float(kmeans.inertia_),
        silhouette=silhouette,
        cluster_sizes={int(k): int(v) for k, v in cluster_sizes.items()},
        mismatch_rate=mismatch_rate,
        majority_mapping={int(k): str(v) for k, v in mapping.items()},
    )


def save_record_level(
    df: pd.DataFrame,
    assignments: np.ndarray,
    distances: np.ndarray,
    path: Path,
) -> Path:
    logging.info("Writing detailed assignments to %s", path)
    path = path.resolve()
    path.parent.mkdir(parents=True, exist_ok=True)
    result = df.copy()
    result["cluster_id"] = assignments.astype(int)
    result["distance_to_centroid"] = distances.astype(float)
    result.to_csv(path, index=False)
    return path


def render_markdown_report(metrics: ClusterMetrics) -> str:
    lines = [
        "# K-Means Report",
        "",
        f"- Clusters (k): {metrics.k}",
        f"- Inertia: {metrics.inertia:.4f}",
        f"- Silhouette (cosine): {metrics.silhouette:.4f}" if metrics.silhouette is not None else "- Silhouette (cosine): N/A",
        f"- Mismatch rate: {metrics.mismatch_rate:.4f}",
        "",
        "| Cluster | Size | Majority Original Group |",
        "| --- | ---: | --- |",
    ]
    for cluster_id, size in sorted(metrics.cluster_sizes.items()):
        majority = metrics.majority_mapping.get(cluster_id, "N/A")
        lines.append(f"| {cluster_id} | {size} | {majority} |")
    return "\n".join(lines) + "\n"


def save_report(metrics: ClusterMetrics, path: Path) -> Path:
    path = path.resolve()
    path.parent.mkdir(parents=True, exist_ok=True)
    if path.suffix.lower() in {".md", ".markdown"}:
        path.write_text(render_markdown_report(metrics), encoding="utf-8")
    else:
        rows = []
        for cluster_id, size in sorted(metrics.cluster_sizes.items()):
            rows.append(
                {
                    "cluster_id": cluster_id,
                    "size": size,
                    "majority_original_group": metrics.majority_mapping.get(cluster_id, ""),
                }
            )
        df = pd.DataFrame(rows)
        df.to_csv(path, index=False)
    logging.info("Wrote report to %s", path)
    return path


def load_manifest(dataset_path: Path, manifest_override: Optional[Path]) -> Optional[Dict[str, Any]]:
    candidate_paths: List[Path] = []
    if manifest_override:
        candidate_paths.append(manifest_override)
    else:
        candidate_paths.append(dataset_path.with_suffix(".manifest.json"))
    for candidate in candidate_paths:
        if candidate.exists():
            try:
                return json.loads(candidate.read_text(encoding="utf-8"))
            except json.JSONDecodeError as exc:
                logging.warning("Failed to parse manifest %s: %s", candidate, exc)
    return None


def save_centroids(path: Path, kmeans: KMeans, manifest: Optional[Dict[str, Any]]) -> Path:
    path = path.resolve()
    path.parent.mkdir(parents=True, exist_ok=True)
    payload: Dict[str, Any] = {
        "k": int(kmeans.n_clusters),
        "centroids": kmeans.cluster_centers_.tolist(),
        "embedding_dim": int(kmeans.cluster_centers_.shape[1]),
        "model_id": manifest.get("model_id") if manifest else None,
        "normalization": manifest.get("normalization") if manifest else "l2",
    }
    path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    logging.info("Persisted centroids to %s", path)
    return path


def main(argv: Optional[List[str]] = None) -> None:
    args = parse_args(argv)
    configure_logging(args.log_level)

    data_path = Path(args.data)
    manifest_path = Path(args.manifest) if args.manifest else None
    if not data_path.exists():
        raise FileNotFoundError(f"Dataset not found: {data_path}")

    df = load_dataset(data_path)
    normalized = dataframe_to_matrix(df, "normalized_embedding")
    titles = df["title"].tolist()
    logging.info("Loaded %d vectors of dimension %d", normalized.shape[0], normalized.shape[1])

    kmeans = KMeans(n_clusters=args.k, random_state=args.seed, n_init="auto")
    assignments = kmeans.fit_predict(normalized)
    centroids = kmeans.cluster_centers_
    distances = np.linalg.norm(normalized - centroids[assignments], axis=1)

    metrics = build_metrics(df, assignments, kmeans)

    logging.info("K-Means complete: inertia=%.4f mismatch_rate=%.4f", metrics.inertia, metrics.mismatch_rate)
    if metrics.silhouette is not None:
        logging.info("Silhouette (cosine)=%.4f", metrics.silhouette)

    if args.output:
        save_record_level(df, assignments, distances, Path(args.output))

    if args.report:
        save_report(metrics, Path(args.report))

    manifest = load_manifest(data_path, manifest_path)
    if args.centroids:
        save_centroids(Path(args.centroids), kmeans, manifest)

    logging.info("Cluster sizes: %s", metrics.cluster_sizes)
    logging.info("Majority group mapping: %s", metrics.majority_mapping)


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:  # noqa: BLE001
        logging.error("Failed: %s", exc)
        sys.exit(1)
